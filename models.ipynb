{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.inspection import permutation_importance\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('combinedData.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Matching\n",
    "\n",
    "Teams that changed their name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_match = {\n",
    "    'CON':'ORL',\n",
    "    'TUL':'DET',\n",
    "    'SAS':'UTA'\n",
    "}\n",
    "\n",
    "team_match_encoded = {\n",
    "    4:12,\n",
    "    20:5,\n",
    "    16:18\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['results', 'seed', 'college', 'collegeOther', 'birthDate', 'win_ratio', 'stint', \n",
    "                      'GS', 'GP', 'rebounds', 'fgAttempted', 'ftAttempted','threeAttempted',\n",
    "                      'minutes', 'pointsFromFieldGoal', \n",
    "                      'pos', 'age',\n",
    "                      'award', 'coachAward',\n",
    "                      'ftMade', 'topg', 'percentage_pointsFromThree', ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding\n",
    "\n",
    "Encoding categorical features and id features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_features = ['tmID', 'playerID', 'coachID']\n",
    "categorical_features = ['confID']\n",
    "df = pd.get_dummies(df, columns=categorical_features)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for feature in id_features:\n",
    "    df[f'{feature}_encoded'] = label_encoder.fit_transform(df[feature])\n",
    "\n",
    "df['playoff'] = df['playoff'].replace({'N': 0, 'Y': 1})\n",
    "\n",
    "\n",
    "new_id_features = id_features.copy()\n",
    "new_id_features.append('year')\n",
    "\n",
    "# map each tmID and its encoded value\n",
    "\n",
    "id_df = df['tmID'].copy()\n",
    "\n",
    "id_df = id_df.reset_index()\n",
    "id_df = id_df.drop(columns=['index'])\n",
    "id_df = id_df.drop_duplicates()\n",
    "id_df = id_df.reset_index()\n",
    "\n",
    "# array of tmIDs\n",
    "tmIDs = id_df['tmID'].to_numpy()\n",
    "print(tmIDs)\n",
    "\n",
    "tmIDS_inverted = {tmid: n for n, tmid in enumerate(tmIDs)}\n",
    "print(tmIDS_inverted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map each playerID and its encoded value\n",
    "player_enc_df = df[['playerID', 'playerID_encoded']].copy()\n",
    "player_enc_df = player_enc_df.reset_index()\n",
    "player_enc_df = player_enc_df.drop(columns=['index'])\n",
    "player_enc_df = player_enc_df.drop_duplicates()\n",
    "player_enc_df = player_enc_df.reset_index()\n",
    "\n",
    "player_enc_dict = player_enc_df.set_index('playerID_encoded').T.to_dict('list')\n",
    "player_enc_dict = {k: v[1] for k, v in player_enc_dict.items()}\n",
    "print(player_enc_dict)\n",
    "\n",
    "#invert the map\n",
    "player_enc_dict_inverted = {v: k for k, v in player_enc_dict.items()}\n",
    "print(player_enc_dict_inverted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map each coachID and its encoded value\n",
    "coach_enc_df = df[['coachID', 'coachID_encoded']].copy()\n",
    "coach_enc_df = coach_enc_df.reset_index()\n",
    "coach_enc_df = coach_enc_df.drop(columns=['index'])\n",
    "coach_enc_df = coach_enc_df.drop_duplicates()\n",
    "coach_enc_df = coach_enc_df.reset_index()\n",
    "\n",
    "coach_enc_dict = coach_enc_df.set_index('coachID_encoded').T.to_dict('list')\n",
    "coach_enc_dict = {k: v[1] for k, v in coach_enc_dict.items()}\n",
    "print(coach_enc_dict)\n",
    "\n",
    "# invert the map\n",
    "coach_enc_dict_inverted = {v: k for k, v in coach_enc_dict.items()}\n",
    "print(coach_enc_dict_inverted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the original not encoded id features\n",
    "df = df.drop(columns=id_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [col for col in df.columns if col not in ['playoff']]\n",
    "print(feature_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Leakage\n",
    "\n",
    "features not to replace with last years:\n",
    " - tmID\n",
    " - year\n",
    " - playerID\n",
    " - stint\n",
    " - pos\n",
    " - height\n",
    " - weight\n",
    " - college\n",
    " - college other\n",
    " - birthDate\n",
    " - age\n",
    " - coachId\n",
    " - coachStint\n",
    " - confID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_replace = [col for col in feature_cols if col not in [\n",
    "    'tmID', 'year', 'playerID', 'stint', 'coachID', 'coachStint', 'confID',\n",
    "    'tmID_encoded', 'playerID_encoded', 'coachID_encoded','confID_EA','confID_WE']]\n",
    "\n",
    "print(features_to_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debug\n",
    "# df.to_csv('featureRemoval.csv')\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format Final Year Results\n",
    "\n",
    "So that it matches the training data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_predictions_df(data, playoffs, certainties):\n",
    "    data = data.reset_index(drop=True)\n",
    "    # dataFrame for the playoffs and certainties\n",
    "    playoffs_df = pd.DataFrame({\n",
    "        'playoff': playoffs,\n",
    "        'certainty': certainties[:, 1] * 100\n",
    "    })\n",
    "\n",
    "    # changing the tmID and playerID to their original values based on the id_df\n",
    "    data['tmID'] = data['tmID_encoded'].apply(lambda x: tmIDs[x])\n",
    "\n",
    "    # concat the data and the playoffs_df\n",
    "    result = pd.concat([data[['tmID', 'confID_EA']], playoffs_df], axis=1)\n",
    "    \n",
    "        \n",
    "    # for every team, find the average certainty and return the teamID and the average certainty and the playoff\n",
    "    result = result.groupby(['tmID']).mean()\n",
    "    result = result.reset_index()\n",
    "\n",
    "    result['playoff'] = result['certainty'].apply(lambda x: 1 if x >= 50 else 0)\n",
    "\n",
    "    result = result.sort_values(by=['confID_EA'], ascending=False)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Year 11 Prep\n",
    "\n",
    "Encoding of year 11 features, matching previous encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year11 = pd.read_csv('dataset11/edited/all.csv')\n",
    "\n",
    "#tmID,year,coachID,coachStint,confID,playerID,stint,lgID\n",
    "year11 = year11.drop(columns=['stint', 'lgID']) \n",
    "\n",
    "year11 = pd.get_dummies(year11, columns=categorical_features)\n",
    "\n",
    "# encode the tmID values based on the tmID_inverted map\n",
    "year11['tmID_encoded'] = year11['tmID'].apply(lambda x: tmIDS_inverted.get(x, 20))\n",
    "\n",
    "year11['playerID_encoded'] = year11['playerID'].apply(lambda x: player_enc_dict_inverted.get(x, 0))\n",
    "\n",
    "year11['coachID_encoded'] = year11['coachID'].apply(lambda x: coach_enc_dict_inverted.get(x, 0))\n",
    "\n",
    "year11 = year11.drop(columns=['tmID', 'playerID', 'coachID'])\n",
    "\n",
    "year11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that predicts the year 11 playoffs, based on a model.\n",
    "Also calculates the certainty of the prediction, formats the dataframe and saves it to a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_playoffs(model, model_name):\n",
    "    data = year11.copy()\n",
    "    # data.to_csv('data_original.csv')\n",
    "    \n",
    "    replace_data = df.copy()\n",
    "    # sort replace data by year descending\n",
    "    replace_data = replace_data.sort_values(by=['year'], ascending=False)\n",
    "    # replace_data.to_csv('replace_data.csv')\n",
    "    # replace data with last years data\n",
    "    key_columns = ['tmID_encoded', 'playerID_encoded']\n",
    "\n",
    "    for index, current_row in data.iterrows():\n",
    "        # replace tmIDs that are in the team_match_encoded map\n",
    "        if current_row[key_columns[0]] in team_match_encoded:\n",
    "            data.at[index, key_columns[0]] = team_match_encoded[current_row[key_columns[0]]]\n",
    "\n",
    "        # Check if there's a matching entry in the previous year's dataframe\n",
    "        matching_entry = replace_data[\n",
    "            (replace_data[key_columns[0]] == current_row[key_columns[0]]) \n",
    "            & (replace_data[key_columns[1]] == current_row[key_columns[1]])\n",
    "            ]\n",
    "\n",
    "        if not matching_entry.empty:\n",
    "            # Replace values in columns_to_replace with data from the matching entry\n",
    "            for column in features_to_replace:\n",
    "                data.at[index, column] = matching_entry.iloc[0][column]\n",
    "        else:\n",
    "            # Remove the row if there is no matching entry\n",
    "            print(f'Removing row {index}, row: {current_row}')\n",
    "            data.drop(index, inplace=True)\n",
    "\n",
    "    \n",
    "    # add entry to the data\n",
    "    # DET,11,lattaiv01w,0,31,1,221,93,4,15,19,20,8,2,16,24,87,34,7,3,49,22,0,7.129032258064516,3.0,0.6451612903225806,0.6129032258064516,0.2580645161290322,0.064516129032258,0.5161290322580645,0.7741935483870968,39.08045977011494,44.89795918367347,42.85714285714285,0.1290322580645161,0.4838709677419355,12.0,12.903225806451612,3.225806451612903,23.655913978494624,G,66.0,143.0,North Carolina,,1984-09-24,39.04,,laimbbi01w,0,24,10,2.4,EA,1,label3,Y,2261,785,442,380,931,1311,563,702,227,564,117,2697,895,2262,547,718,203,610,309,780,1089,546,767,276,529,117,2540,0.706,0.706,0.706,6850,42.99,75.8,35.75,39.57,76.18,33.28,0\n",
    "    \n",
    "    new_row = {\n",
    "        'year': 11,'points':93,'oRebounds':4,'dRebounds':15,'assists':20,'steals':8,'blocks':2,'turnovers':16,'PF':24,'fgMade':34,'threeMade':22,'dq':0,'mpg':7.13,'ppg':3,'apg':0.65,'rpg':0.61,'spg':0.26,'bpg':0.06,'pfpg':0.77,'fg%':39.08,'3p%':44.9,'ft%':42.86,'orpg':0.13,'drpg':0.48,'percentage_pointsFromFieldGoal':12.9,'percentage_pointsFromFreeThrow':3.29,'height':66,'weight':143,'coachStint':0,'coachWon':24,'coachLost':10,'coach W/L Ratio':2.4,'o_fga':2261,'o_fta':785,'o_3pa':442,'o_oreb':380,'o_dreb':931,'o_reb':1311,'o_asts':563,'o_pf':702,'o_stl':227,'o_to':564,'o_blk':117,'o_pts':2697,'d_fgm':895,'d_fga':2262,'d_ftm':547,'d_fta':718,'d_3pm':203,'d_3pa':610,'d_oreb':309,'d_dreb':780,'d_reb':1089,'d_asts':546,'d_pf':767,'d_stl':276,'d_to':529,'d_blk':117,'d_pts':2540,'homeW_ratio':0.71,'awayW_ratio':0.71,'min':6850,'o_fg%':42.99,'o_ft%':75.8,'o_3p%':35.75,'d_fg%':39.57,'d_ft%':76.18,'d_3p%':33.28,'confID_EA':False,'confID_WE':True,'tmID_encoded':5,'playerID_encoded':268,'coachID_encoded':134\n",
    "    }\n",
    "    \n",
    "    # sort the features so that they match df\n",
    "    data = data[feature_cols]\n",
    "    \n",
    "    data = pd.concat([data, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    # data.to_csv('data.csv')\n",
    "\n",
    "    # Predict the classes\n",
    "    predictions = model.predict(data)\n",
    "    \n",
    "    # Predict the probabilities\n",
    "    certainties = model.predict_proba(data)\n",
    "\n",
    "    result = create_predictions_df(data, predictions, certainties)\n",
    "    result.to_csv(f'predictions/{model_name}_predictions.csv')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importance_graph(importances, X_train):\n",
    "    # Sort feature importances in descending order\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    # Rearrange feature names so they match the sorted feature importances\n",
    "    names = [X_train.columns[i] for i in indices]\n",
    "\n",
    "    # Create plot\n",
    "    plt.figure(figsize=(18, 6))\n",
    "\n",
    "    # Create plot title\n",
    "    plt.title(\"Feature Importance\")\n",
    "\n",
    "    # Add bars\n",
    "    plt.bar(range(X_train.shape[1]), importances[indices])\n",
    "\n",
    "    # Add feature names as x-axis labels\n",
    "    plt.xticks(range(X_train.shape[1]), names, rotation=90)\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start window with the first 5 years of training data\n",
    "yearsInit = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = [i for i in range(1, 20)]\n",
    "max_features = [None, 'sqrt', 'log2']\n",
    "\n",
    "best_accuracies = []\n",
    "best_precisions = []\n",
    "best_recalls = []\n",
    "best_f1s = []\n",
    "best_accuracy = 0\n",
    "best_model = None\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for depth in max_depth:\n",
    "    for feature in max_features:\n",
    "        accuracies = []\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        f1s = []\n",
    "        model = DecisionTreeClassifier(max_depth=depth, max_features=feature)\n",
    "\n",
    "        for i in range(yearsInit, 11):\n",
    "            train_years = range(1,i)\n",
    "            test_year = i\n",
    "\n",
    "            train_data = df[df['year'].isin(train_years)]\n",
    "            replace_data = df[df['year'] == test_year-1]\n",
    "            test_data = df[df['year'] == test_year]\n",
    "\n",
    "            # replace data with last years data\n",
    "            key_columns = ['tmID_encoded', 'playerID_encoded']\n",
    "            \n",
    "            for index, current_row in test_data.iterrows():\n",
    "                common_key = tuple(current_row[key_columns])\n",
    "\n",
    "                # replace tmIDs that are in the team_match_encoded dictionary\n",
    "                if current_row[key_columns[0]] in team_match_encoded:\n",
    "                    test_data.at[index, key_columns[0]] = team_match_encoded[current_row[key_columns[0]]]\n",
    "\n",
    "                # Check if there's a matching entry in the previous year's dataframe\n",
    "                matching_entry = replace_data[(replace_data[key_columns[0]] == current_row[key_columns[0]]) & (replace_data[key_columns[1]] == current_row[key_columns[1]])]\n",
    "\n",
    "                if not matching_entry.empty:\n",
    "                    # Replace values in columns_to_replace with data from the matching entry\n",
    "                    for column in features_to_replace:\n",
    "                        test_data.at[index, column] = matching_entry.iloc[0][column]\n",
    "                # check if the team is in the list of teams that changed names\n",
    "                elif current_row[key_columns[0]] in team_match_encoded:\n",
    "                    # replace the teamID with the new teamID\n",
    "                    test_data.at[index, key_columns[0]] = team_match_encoded[current_row[key_columns[0]]]\n",
    "                else:\n",
    "                    # Remove the row if there is no matching entry\n",
    "                    test_data.drop(index, inplace=True)\n",
    "\n",
    "            y_train = train_data['playoff']\n",
    "            y_test = test_data['playoff']\n",
    "\n",
    "            # Prepare the data for training and testing\n",
    "            X_train, X_test = train_data[feature_cols], test_data[feature_cols]\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # Make predictions for individual players\n",
    "            player_predictions = model.predict(X_test)\n",
    "            prediction_probabilities = model.predict_proba(X_test)\n",
    "            prediction_probabilities = np.around(prediction_probabilities, decimals=4)\n",
    "            \n",
    "            pred_df = create_predictions_df(X_test, player_predictions, prediction_probabilities)\n",
    "\n",
    "            # Evaluate the model\n",
    "            accuracy = accuracy_score(y_test, player_predictions)\n",
    "            precision = precision_score(y_test, player_predictions)\n",
    "            recall = recall_score(y_test, player_predictions)\n",
    "            f1 = recall_score(y_test, player_predictions)\n",
    "            accuracies.append(accuracy)\n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "            f1s.append(f1)\n",
    "            \n",
    "        accuracy_average = round(sum(accuracies)/len(accuracies), 3)\n",
    "        if accuracy_average > best_accuracy:\n",
    "            best_accuracy = accuracy_average\n",
    "            best_model = model\n",
    "            best_accuracies = accuracies\n",
    "            best_precisions = precisions\n",
    "            best_recalls = recalls\n",
    "            best_f1s = f1s\n",
    "\n",
    "duration = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = create_predictions_df(X_test, player_predictions, prediction_probabilities)\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'Accuracy': best_accuracies, 'Precision': best_precisions, 'Recall': best_recalls, 'F1 Score': best_f1s})\n",
    "# Create a box plot\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 1, 1)  # Create a single subplot for the box plot\n",
    "sns.boxplot(data=data, palette=\"Set3\")\n",
    "plt.title(\"Decision Tree Metrics Box Plot\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_average = round(sum(best_accuracies)/len(best_accuracies), 3)\n",
    "precision_average = round(sum(best_precisions)/len(best_precisions), 3)\n",
    "recall_average = round(sum(best_recalls)/len(best_recalls), 3)\n",
    "f1s_average = round(sum(best_f1s)/len(best_f1s), 3)\n",
    "\n",
    "print(f\"Accuracy Average: {accuracy_average*100}%\")\n",
    "print(f\"Precision Average: {precision_average*100}%\")\n",
    "print(f\"Recall Average: {recall_average*100}%\")\n",
    "print(f\"F1 Average: {f1s_average*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Duration: {duration} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debug\n",
    "# dot_data = export_graphviz(best_model, out_file=None,\n",
    "#                           feature_names=X_train.columns, # Specify your feature names\n",
    "#                           class_names=['No Playoff', 'Playoff'], # Specify your class names\n",
    "#                           filled=True, rounded=True, special_characters=True)\n",
    "\n",
    "# graph = graphviz.Source(dot_data)\n",
    "# graph.render(\"decision_tree\") # This will save the tree as 'decision_tree.pdf'\n",
    "#graph.view(\"decision_tree\")   # This will open the tree in your default PDF viewer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most important attributes\n",
    "importances = best_model.feature_importances_\n",
    "\n",
    "importance_graph(importances, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Year 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict_playoffs(best_model, 'decision_tree')\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depth = [i for i in range(1, 20)]\n",
    "max_features =  [None, 'sqrt', 'log2']\n",
    "\n",
    "best_accuracies = []\n",
    "best_precisions = []\n",
    "best_recalls = []\n",
    "best_f1s = []\n",
    "best_accuracy = 0\n",
    "best_model = None\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for depth in max_depth:\n",
    "    for feature in max_features:\n",
    "        accuracies = []\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        f1s = []\n",
    "        model = RandomForestClassifier(max_depth=depth, max_features=feature)\n",
    "        for i in range(yearsInit, 11):\n",
    "            train_years = range(1,i)\n",
    "            test_year = i\n",
    "\n",
    "            train_data = df[df['year'].isin(train_years)]\n",
    "            replace_data = df[df['year'] == test_year-1]\n",
    "            test_data = df[df['year'] == test_year]\n",
    "\n",
    "            # replace data with last years data\n",
    "            key_columns = ['tmID_encoded', 'playerID_encoded']\n",
    "            \n",
    "            for index, current_row in test_data.iterrows():\n",
    "                common_key = tuple(current_row[key_columns])\n",
    "\n",
    "                # Check if there's a matching entry in the previous year's dataframe\n",
    "                matching_entry = replace_data[(replace_data[key_columns[0]] == current_row[key_columns[0]]) & (replace_data[key_columns[1]] == current_row[key_columns[1]])]\n",
    "\n",
    "                if not matching_entry.empty:\n",
    "                    # Replace values in columns_to_replace with data from the matching entry\n",
    "                    for column in features_to_replace:\n",
    "                        test_data.at[index, column] = matching_entry.iloc[0][column]\n",
    "                # check if the team is in the list of teams that changed names\n",
    "                elif current_row[key_columns[0]] in team_match_encoded:\n",
    "                    # replace the teamID with the new teamID\n",
    "                    test_data.at[index, key_columns[0]] = team_match_encoded[current_row[key_columns[0]]]\n",
    "                else:\n",
    "                    # Remove the row if there is no matching entry\n",
    "                    test_data.drop(index, inplace=True)\n",
    "\n",
    "            y_train = train_data['playoff']\n",
    "            y_test = test_data['playoff']\n",
    "\n",
    "            # Prepare the data for training and testing\n",
    "            X_train, X_test = train_data[feature_cols], test_data[feature_cols]\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # Make predictions for individual players\n",
    "            player_predictions = model.predict(X_test)\n",
    "            prediction_probabilities = model.predict_proba(X_test)\n",
    "            prediction_probabilities = np.around(prediction_probabilities, decimals=4)\n",
    "\n",
    "            # Evaluate the model\n",
    "            accuracy = accuracy_score(y_test, player_predictions)\n",
    "            precision = precision_score(y_test, player_predictions)\n",
    "            recall = recall_score(y_test, player_predictions)\n",
    "            f1 = recall_score(y_test, player_predictions)\n",
    "            accuracies.append(accuracy)\n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "            f1s.append(f1)\n",
    "        \n",
    "        accuracy_average = round(sum(accuracies)/len(accuracies), 3)\n",
    "        if accuracy_average > best_accuracy:\n",
    "            best_accuracy = accuracy_average\n",
    "            best_model = model\n",
    "            best_accuracies = accuracies\n",
    "            best_precisions = precisions\n",
    "            best_recalls = recalls\n",
    "            best_f1s = f1s\n",
    "\n",
    "duration = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = create_predictions_df(X_test, player_predictions, prediction_probabilities)\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'Accuracy': best_accuracies, 'Precision': best_precisions, 'Recall': best_recalls, 'F1 Score': best_f1s})\n",
    "# Create a box plot\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 1, 1)  # Create a single subplot for the box plot\n",
    "sns.boxplot(data=data, palette=\"Set3\")\n",
    "plt.title(\"Random Forest Metrics Box Plot\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_average = round(sum(best_accuracies)/len(best_accuracies), 3)\n",
    "precision_average = round(sum(best_precisions)/len(best_precisions), 3)\n",
    "recall_average = round(sum(best_recalls)/len(best_recalls), 3)\n",
    "f1s_average = round(sum(best_f1s)/len(best_f1s), 3)\n",
    "\n",
    "print(f\"Accuracy Average: {accuracy_average*100}%\")\n",
    "print(f\"Precision Average: {precision_average*100}%\")\n",
    "print(f\"Recall Average: {recall_average*100}%\")\n",
    "print(f\"F1 Average: {f1s_average*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Duration: {duration} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature importance\n",
    "importances = best_model.feature_importances_\n",
    "importance_graph(importances, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Year 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict_playoffs(best_model, 'random_forest')\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = [0.1, 1, 10, 100, 1000]\n",
    "gamma = ['scale', 'auto']  # Kernel coefficient\n",
    "kernel = ['linear', 'rbf', 'poly', 'sigmoid']  # Type of SVM\n",
    "\n",
    "best_accuracies = []\n",
    "best_precisions = []\n",
    "best_recalls = []\n",
    "best_f1s = []\n",
    "best_accuracy = 0\n",
    "best_model = None\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for c in C:\n",
    "    for g in gamma:\n",
    "        for k in kernel:\n",
    "            accuracies = []\n",
    "            precisions = []\n",
    "            recalls = []\n",
    "            f1s = []\n",
    "            model = SVC(kernel=k, C=c, gamma=g, probability=True)\n",
    "\n",
    "            for i in range(yearsInit, 11):\n",
    "                train_years = range(1,i)\n",
    "                test_year = i\n",
    "\n",
    "                train_data = df[df['year'].isin(train_years)]\n",
    "                replace_data = df[df['year'] == test_year-1]\n",
    "                test_data = df[df['year'] == test_year]\n",
    "\n",
    "                # replace data with last years data\n",
    "                key_columns = ['tmID_encoded', 'playerID_encoded']\n",
    "                \n",
    "                for index, current_row in test_data.iterrows():\n",
    "                    common_key = tuple(current_row[key_columns])\n",
    "\n",
    "                    # Check if there's a matching entry in the previous year's dataframe\n",
    "                    matching_entry = replace_data[(replace_data[key_columns[0]] == current_row[key_columns[0]]) & (replace_data[key_columns[1]] == current_row[key_columns[1]])]\n",
    "\n",
    "                    if not matching_entry.empty:\n",
    "                        # Replace values in columns_to_replace with data from the matching entry\n",
    "                        for column in features_to_replace:\n",
    "                            test_data.at[index, column] = matching_entry.iloc[0][column]\n",
    "                    # check if the team is in the list of teams that changed names\n",
    "                    elif current_row[key_columns[0]] in team_match_encoded:\n",
    "                        # replace the teamID with the new teamID\n",
    "                        test_data.at[index, key_columns[0]] = team_match_encoded[current_row[key_columns[0]]]\n",
    "                    else:\n",
    "                        # Remove the row if there is no matching entry\n",
    "                        test_data.drop(index, inplace=True)\n",
    "\n",
    "                y_train = train_data['playoff']\n",
    "                y_test = test_data['playoff']\n",
    "\n",
    "                # Prepare the data for training and testing\n",
    "                X_train, X_test = train_data[feature_cols], test_data[feature_cols]\n",
    "\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                # Make predictions for individual players\n",
    "                player_predictions = model.predict(X_test)\n",
    "                prediction_probabilities = model.predict_proba(X_test)\n",
    "\n",
    "                \n",
    "                # convert pred_array to 4 decimal places\n",
    "                prediction_probabilities = np.around(prediction_probabilities, decimals=4)\n",
    "\n",
    "                # Evaluate the model\n",
    "                accuracy = accuracy_score(y_test, player_predictions)\n",
    "                precision = precision_score(y_test, player_predictions)\n",
    "                recall = recall_score(y_test, player_predictions)\n",
    "                f1 = recall_score(y_test, player_predictions)\n",
    "                accuracies.append(accuracy)\n",
    "                precisions.append(precision)\n",
    "                recalls.append(recall)\n",
    "                f1s.append(f1)\n",
    "                print(f\"year {i} done\")\n",
    "            \n",
    "            accuracy_average = round(sum(accuracies)/len(accuracies), 3)\n",
    "            if accuracy_average > best_accuracy:\n",
    "                best_accuracy = accuracy_average\n",
    "                best_model = model\n",
    "                best_accuracies = accuracies\n",
    "                best_precisions = precisions\n",
    "                best_recalls = recalls\n",
    "                best_f1s = f1s\n",
    "\n",
    "duration = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = create_predictions_df(X_test, player_predictions, prediction_probabilities)\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'Accuracy': best_accuracies, 'Precision': best_precisions, 'Recall': best_recalls, 'F1 Score': best_f1s})\n",
    "# Create a box plot\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 1, 1)  # Create a single subplot for the box plot\n",
    "sns.boxplot(data=data, palette=\"Set3\")\n",
    "plt.title(\"SVC Metrics Box Plot\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_average = round(sum(best_accuracies)/len(best_accuracies), 3)\n",
    "precision_average = round(sum(best_precisions)/len(best_precisions), 3)\n",
    "recall_average = round(sum(best_recalls)/len(best_recalls), 3)\n",
    "f1s_average = round(sum(best_f1s)/len(best_f1s), 3)\n",
    "\n",
    "print(f\"Accuracy Average: {accuracy_average*100}%\")\n",
    "print(f\"Precision Average: {precision_average*100}%\")\n",
    "print(f\"Recall Average: {recall_average*100}%\")\n",
    "print(f\"F1 Average: {f1s_average*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Duration: {duration} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature importance graph\n",
    "perm = PermutationImportance(best_model, random_state=1).fit(X_test, y_test)\n",
    "eli5.show_weights(perm, feature_names=X_test.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Year 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict_playoffs(best_model, 'svm')\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors = [i for i in range(1, 20)]\n",
    "weights = ['uniform', 'distance']\n",
    "metric = ['euclidean', 'manhattan', 'minkowski']\n",
    "\n",
    "best_accuracies = []\n",
    "best_precisions = []\n",
    "best_recalls = []\n",
    "best_f1s = []\n",
    "best_accuracy = 0\n",
    "best_model = None\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for n in n_neighbors:\n",
    "    for w in weights:\n",
    "        for m in metric:\n",
    "            accuracies = []\n",
    "            precisions = []\n",
    "            recalls = []\n",
    "            f1s = []\n",
    "            model = KNeighborsClassifier(n_neighbors=n, weights=w, metric=m)\n",
    "\n",
    "            for i in range(yearsInit, 11):\n",
    "                train_years = range(1,i)\n",
    "                test_year = i\n",
    "\n",
    "                train_data = df[df['year'].isin(train_years)]\n",
    "                \n",
    "                \n",
    "                replace_data = df[df['year'] == test_year-1]\n",
    "                test_data = df[df['year'] == test_year]\n",
    "                \n",
    "                \n",
    "\n",
    "                # replace data with last years data\n",
    "                key_columns = ['tmID_encoded', 'playerID_encoded']\n",
    "                \n",
    "                for index, current_row in test_data.iterrows():\n",
    "                    common_key = tuple(current_row[key_columns])\n",
    "\n",
    "                    # Check if there's a matching entry in the previous year's dataframe\n",
    "                    matching_entry = replace_data[(replace_data[key_columns[0]] == current_row[key_columns[0]]) & (replace_data[key_columns[1]] == current_row[key_columns[1]])]\n",
    "\n",
    "                    if not matching_entry.empty:\n",
    "                        # Replace values in columns_to_replace with data from the matching entry\n",
    "                        for column in features_to_replace:\n",
    "                            test_data.at[index, column] = matching_entry.iloc[0][column]\n",
    "                    # check if the team is in the list of teams that changed names\n",
    "                    elif current_row[key_columns[0]] in team_match_encoded:\n",
    "                        # replace the teamID with the new teamID\n",
    "                        test_data.at[index, key_columns[0]] = team_match_encoded[current_row[key_columns[0]]]\n",
    "                    else:\n",
    "                        # Remove the row if there is no matching entry\n",
    "                        test_data.drop(index, inplace=True)\n",
    "                \n",
    "                \n",
    "\n",
    "                y_train = train_data['playoff']\n",
    "                y_test = test_data['playoff']\n",
    "\n",
    "                # Prepare the data for training and testing\n",
    "                X_train, X_test = train_data[feature_cols], test_data[feature_cols]\n",
    "\n",
    "                \n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                # Make predictions for individual players\n",
    "                player_predictions = model.predict(X_test)\n",
    "                prediction_probabilities = model.predict_proba(X_test)\n",
    "                prediction_probabilities = np.around(prediction_probabilities, decimals=4)\n",
    "\n",
    "\n",
    "                # Evaluate the model\n",
    "                accuracy = accuracy_score(y_test, player_predictions)\n",
    "                precision = precision_score(y_test, player_predictions)\n",
    "                recall = recall_score(y_test, player_predictions)\n",
    "                f1 = recall_score(y_test, player_predictions)\n",
    "                accuracies.append(accuracy)\n",
    "                precisions.append(precision)\n",
    "                recalls.append(recall)\n",
    "                f1s.append(f1)\n",
    "                print(f\"year {i} done\")\n",
    "\n",
    "            accuracy_average = round(sum(accuracies)/len(accuracies), 3)\n",
    "            if accuracy_average > best_accuracy:\n",
    "                best_accuracy = accuracy_average\n",
    "                best_model = model\n",
    "                best_accuracies = accuracies\n",
    "                best_precisions = precisions\n",
    "                best_recalls = recalls\n",
    "                best_f1s = f1s\n",
    "                \n",
    "duration = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = create_predictions_df(X_test, player_predictions, prediction_probabilities)\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'Accuracy': best_accuracies, 'Precision': best_precisions, 'Recall': best_recalls, 'F1 Score': best_f1s})\n",
    "# Create a box plot\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 1, 1)  # Create a single subplot for the box plot\n",
    "sns.boxplot(data=data, palette=\"Set3\")\n",
    "plt.title(\"KNN Metrics Box Plot\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_average = round(sum(best_accuracies)/len(best_accuracies), 3)\n",
    "precision_average = round(sum(best_precisions)/len(best_precisions), 3)\n",
    "recall_average = round(sum(best_recalls)/len(best_recalls), 3)\n",
    "f1s_average = round(sum(best_f1s)/len(best_f1s), 3)\n",
    "\n",
    "print(f\"Accuracy Average: {accuracy_average*100}%\")\n",
    "print(f\"Precision Average: {precision_average*100}%\")\n",
    "print(f\"Recall Average: {recall_average*100}%\")\n",
    "print(f\"F1 Average: {f1s_average*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Duration: {duration} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importance graph\n",
    "results = permutation_importance(best_model, X_test, y_test, scoring='accuracy')\n",
    "importances = results.importances_mean\n",
    "importance_graph(importances, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Year 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict_playoffs(best_model, 'nearest_neighbors')\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_smoothing = [1e-09, 1e-08, 1e-07, 1e-06, 1e-05]\n",
    "\n",
    "best_accuracies = []\n",
    "best_precisions = []\n",
    "best_recalls = []\n",
    "best_f1s = []\n",
    "best_accuracy = 0\n",
    "best_model = None\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for v in var_smoothing:\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    model = GaussianNB(var_smoothing=v)\n",
    "\n",
    "    for i in range(yearsInit, 11):\n",
    "        train_years = range(1,i)\n",
    "        test_year = i\n",
    "\n",
    "        train_data = df[df['year'].isin(train_years)]\n",
    "        \n",
    "        \n",
    "        replace_data = df[df['year'] == test_year-1]\n",
    "        test_data = df[df['year'] == test_year]\n",
    "        \n",
    "        \n",
    "\n",
    "        # replace data with last years data\n",
    "        key_columns = ['tmID_encoded', 'playerID_encoded']\n",
    "        \n",
    "        for index, current_row in test_data.iterrows():\n",
    "            common_key = tuple(current_row[key_columns])\n",
    "\n",
    "            # Check if there's a matching entry in the previous year's dataframe\n",
    "            matching_entry = replace_data[(replace_data[key_columns[0]] == current_row[key_columns[0]]) & (replace_data[key_columns[1]] == current_row[key_columns[1]])]\n",
    "\n",
    "            if not matching_entry.empty:\n",
    "                # Replace values in columns_to_replace with data from the matching entry\n",
    "                for column in features_to_replace:\n",
    "                    test_data.at[index, column] = matching_entry.iloc[0][column]\n",
    "            # check if the team is in the list of teams that changed names\n",
    "            elif current_row[key_columns[0]] in team_match_encoded:\n",
    "                # replace the teamID with the new teamID\n",
    "                test_data.at[index, key_columns[0]] = team_match_encoded[current_row[key_columns[0]]]\n",
    "            else:\n",
    "                # Remove the row if there is no matching entry\n",
    "                test_data.drop(index, inplace=True)\n",
    "        \n",
    "        \n",
    "\n",
    "        y_train = train_data['playoff']\n",
    "        y_test = test_data['playoff']\n",
    "\n",
    "        # Prepare the data for training and testing\n",
    "        X_train, X_test = train_data[feature_cols], test_data[feature_cols]\n",
    "\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions for individual players\n",
    "        player_predictions = model.predict(X_test)\n",
    "        prediction_probabilities = model.predict_proba(X_test)\n",
    "        prediction_probabilities = np.around(prediction_probabilities, decimals=4)\n",
    "\n",
    "\n",
    "        # Evaluate the model\n",
    "        accuracy = accuracy_score(y_test, player_predictions)\n",
    "        precision = precision_score(y_test, player_predictions)\n",
    "        recall = recall_score(y_test, player_predictions)\n",
    "        f1 = recall_score(y_test, player_predictions)\n",
    "        accuracies.append(accuracy)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "        print(f\"year {i} done\")\n",
    "    accuracy_average = round(sum(accuracies)/len(accuracies), 3)\n",
    "    if accuracy_average > best_accuracy:\n",
    "        best_accuracy = accuracy_average\n",
    "        best_model = model\n",
    "        best_accuracies = accuracies\n",
    "        best_precisions = precisions\n",
    "        best_recalls = recalls\n",
    "        best_f1s = f1s\n",
    "        \n",
    "duration = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = create_predictions_df(X_test, player_predictions, prediction_probabilities)\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'Accuracy': best_accuracies, 'Precision': best_precisions, 'Recall': best_recalls, 'F1 Score': best_f1s})\n",
    "# Create a box plot\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 1, 1)  # Create a single subplot for the box plot\n",
    "sns.boxplot(data=data, palette=\"Set3\")\n",
    "plt.title(\"Naive Bayes Metrics Box Plot\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_average = round(sum(best_accuracies)/len(best_accuracies), 3)\n",
    "precision_average = round(sum(best_precisions)/len(best_precisions), 3)\n",
    "recall_average = round(sum(best_recalls)/len(best_recalls), 3)\n",
    "f1s_average = round(sum(best_f1s)/len(best_f1s), 3)\n",
    "\n",
    "print(f\"Accuracy Average: {accuracy_average*100}%\")\n",
    "print(f\"Precision Average: {precision_average*100}%\")\n",
    "print(f\"Recall Average: {recall_average*100}%\")\n",
    "print(f\"F1 Average: {f1s_average*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Duration: {duration} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature importance\n",
    "importances = best_model.theta_[0]\n",
    "importance_graph(importances, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Year 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict_playoffs(best_model, 'naive_bayes')\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_accuracies = []\n",
    "best_precisions = []\n",
    "best_recalls = []\n",
    "best_f1s = []\n",
    "best_accuracy = 0\n",
    "best_model = None\n",
    "\n",
    "\n",
    "layers = [(x,) for x in range(25, 200, 25)] \n",
    "layers += [(x, y) for x in range(25, 200, 25) for y in range(25, 200, 25)] \n",
    "\n",
    "hidden_layer_sizes = layers\n",
    "activation =  ['relu', 'logistic', 'tanh']\n",
    "alpha = [0.0001, 0.001, 0.01]\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for h in hidden_layer_sizes:\n",
    "    for a in activation:\n",
    "        for alp in alpha:\n",
    "            accuracies = []\n",
    "            precisions = []\n",
    "            recalls = []\n",
    "            f1s = []\n",
    "            model = MLPClassifier(max_iter=500, learning_rate_init=0.001, batch_size=64, hidden_layer_sizes=h, activation=a, alpha=alp)\n",
    "\n",
    "            for i in range(yearsInit, 11):\n",
    "                train_years = range(1,i)\n",
    "                test_year = i\n",
    "\n",
    "                train_data = df[df['year'].isin(train_years)]\n",
    "                \n",
    "                \n",
    "                replace_data = df[df['year'] == test_year-1]\n",
    "                test_data = df[df['year'] == test_year]\n",
    "                \n",
    "                \n",
    "\n",
    "                # replace data with last years data\n",
    "                key_columns = ['tmID_encoded', 'playerID_encoded']\n",
    "                \n",
    "                for index, current_row in test_data.iterrows():\n",
    "                    common_key = tuple(current_row[key_columns])\n",
    "\n",
    "                    # Check if there's a matching entry in the previous year's dataframe\n",
    "                    matching_entry = replace_data[(replace_data[key_columns[0]] == current_row[key_columns[0]]) & (replace_data[key_columns[1]] == current_row[key_columns[1]])]\n",
    "\n",
    "                    if not matching_entry.empty:\n",
    "                        # Replace values in columns_to_replace with data from the matching entry\n",
    "                        for column in features_to_replace:\n",
    "                            test_data.at[index, column] = matching_entry.iloc[0][column]\n",
    "                    # check if the team is in the list of teams that changed names\n",
    "                    elif current_row[key_columns[0]] in team_match_encoded:\n",
    "                        # replace the teamID with the new teamID\n",
    "                        test_data.at[index, key_columns[0]] = team_match_encoded[current_row[key_columns[0]]]\n",
    "                    else:\n",
    "                        # Remove the row if there is no matching entry\n",
    "                        test_data.drop(index, inplace=True)\n",
    "                \n",
    "                \n",
    "\n",
    "                y_train = train_data['playoff']\n",
    "                y_test = test_data['playoff']\n",
    "\n",
    "                # Prepare the data for training and testing\n",
    "                X_train, X_test = train_data[feature_cols], test_data[feature_cols]\n",
    "\n",
    "                \n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                # Make predictions for individual players\n",
    "                player_predictions = model.predict(X_test)\n",
    "                prediction_probabilities = model.predict_proba(X_test)\n",
    "                prediction_probabilities = np.around(prediction_probabilities, decimals=4)\n",
    "\n",
    "\n",
    "                # Evaluate the model\n",
    "                accuracy = accuracy_score(y_test, player_predictions)\n",
    "                precision = precision_score(y_test, player_predictions)\n",
    "                recall = recall_score(y_test, player_predictions)\n",
    "                f1 = recall_score(y_test, player_predictions)\n",
    "                accuracies.append(accuracy)\n",
    "                precisions.append(precision)\n",
    "                recalls.append(recall)\n",
    "                f1s.append(f1)\n",
    "                print(f\"year {i} done\")\n",
    "            accuracy_average = round(sum(accuracies)/len(accuracies), 3)\n",
    "            if accuracy_average > best_accuracy:\n",
    "                best_accuracy = accuracy_average\n",
    "                best_model = model\n",
    "                best_accuracies = accuracies\n",
    "                best_precisions = precisions\n",
    "                best_recalls = recalls\n",
    "                best_f1s = f1s\n",
    "                \n",
    "duration = time.time() - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = create_predictions_df(X_test, player_predictions, prediction_probabilities)\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'Accuracy': best_accuracies, 'Precision': best_precisions, 'Recall': best_recalls, 'F1 Score': best_f1s})\n",
    "# Create a box plot\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 1, 1)  # Create a single subplot for the box plot\n",
    "sns.boxplot(data=data, palette=\"Set3\")\n",
    "plt.title(\"Neural Network Metrics Box Plot\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_average = round(sum(best_accuracies)/len(best_accuracies), 3)\n",
    "precision_average = round(sum(best_precisions)/len(best_precisions), 3)\n",
    "recall_average = round(sum(best_recalls)/len(best_recalls), 3)\n",
    "f1s_average = round(sum(best_f1s)/len(best_f1s), 3)\n",
    "\n",
    "print(f\"Accuracy Average: {accuracy_average*100}%\")\n",
    "print(f\"Precision Average: {precision_average*100}%\")\n",
    "print(f\"Recall Average: {recall_average*100}%\")\n",
    "print(f\"F1 Average: {f1s_average*100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Duration: {duration} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature importance graph\n",
    "perm = PermutationImportance(best_model, random_state=1).fit(X_test, y_test)\n",
    "eli5.show_weights(perm, feature_names = X_test.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Year 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict_playoffs(best_model, 'neural_network')\n",
    "pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
